"""
Z-scoreæ ‡å‡†åŒ–æ¨¡å—

å°†æŒ‡å®šåˆ—çš„æ•°æ®ç¼©æ”¾ä¸ºå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒ

å‚æ•°:
data : pd.DataFrameï¼Œè¾“å…¥çš„DataFrameæ•°æ®
columns : str, int, or List[Union[str, int]]ï¼Œéœ€è¦æ ‡å‡†åŒ–çš„åˆ—åæˆ–åˆ—ç´¢å¼•ï¼Œå¯ä»¥æ˜¯å•ä¸ªå€¼æˆ–åˆ—è¡¨
with_mean : bool, default=Trueï¼Œæ˜¯å¦å‡å»å‡å€¼
with_std : bool, default=Trueï¼Œæ˜¯å¦é™¤ä»¥æ ‡å‡†å·®

è¿”å›:
Tuple[pd.DataFrame, dict]
æ ‡å‡†åŒ–åçš„DataFrame
åŒ…å«å„åˆ—ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸ {'åˆ—å': {'mean': å‡å€¼, 'std': æ ‡å‡†å·®}}

å¼‚å¸¸:
ValueError: è¾“å…¥å‚æ•°é”™è¯¯
KeyError: åˆ—åä¸å­˜åœ¨
TypeError: æ•°æ®ç±»å‹é”™è¯¯
"""
# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆä¿®æ­£ç‰ˆæœ¬ï¼‰
    np.random.seed(42)
    test_data = pd.DataFrame({
        'age': np.random.randint(18, 80, 100),
        'income': np.random.normal(50000, 15000, 100),
        'score': np.random.uniform(0, 100, 100)
    })

    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
    test_data.loc[5:10, 'age'] = np.nan
    test_data.loc[15:20, 'income'] = np.nan

    print("åŸå§‹æ•°æ®:")
    print(test_data.head(10))
    print("\næ•°æ®ä¿¡æ¯:")
    print(test_data.info())

    try:
        # æµ‹è¯•1: å•åˆ—æ ‡å‡†åŒ–ï¼ˆageåˆ—ï¼‰
        print("\n=== æµ‹è¯•å•åˆ—æ ‡å‡†åŒ– ===")
        result1, stats1 = zscore_standardization(test_data, 'age')
        print("æ ‡å‡†åŒ–åçš„ageåˆ—å‰15è¡Œ:")
        print(result1[['age']].head(15))
        print("ç»Ÿè®¡ä¿¡æ¯:", stats1)

        # æµ‹è¯•2: å¤šåˆ—æ ‡å‡†åŒ–
        print("\n=== æµ‹è¯•å¤šåˆ—æ ‡å‡†åŒ– ===")
        result2, stats2 = zscore_standardization(test_data, ['income', 'score'])
        print("æ ‡å‡†åŒ–ç»“æœ:")
        print(result2[['income', 'score']].head(10))
        print("ç»Ÿè®¡ä¿¡æ¯:", stats2)

        # æµ‹è¯•3: ä½¿ç”¨åˆ—ç´¢å¼•
        print("\n=== æµ‹è¯•ä½¿ç”¨åˆ—ç´¢å¼• ===")
        result3, stats3 = zscore_standardization(test_data, [0, 2])  # ageå’Œscoreåˆ—
        print("ä½¿ç”¨ç´¢å¼•æ ‡å‡†åŒ–ç»“æœ:")
        print(result3[['age', 'score']].head(10))
        print("ç»Ÿè®¡ä¿¡æ¯:", stats3)

        # æµ‹è¯•4: éªŒè¯æ ‡å‡†åŒ–ç»“æœ
        print("\n=== éªŒè¯æ ‡å‡†åŒ–ç»“æœ ===")
        for col in ['age', 'income', 'score']:
            if col in result2.columns:
                valid_data = result2[col].dropna()
                print(f"{col}åˆ— - å‡å€¼: {valid_data.mean():.6f}, æ ‡å‡†å·®: {valid_data.std():.6f}")

    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

    # æµ‹è¯•å¼‚å¸¸æƒ…å†µ
    print("\n=== å¼‚å¸¸æµ‹è¯• ===")

    try:
        # åˆ›å»ºä¸€ä¸ªåŒ…å«éæ•°å€¼åˆ—çš„æµ‹è¯•æ•°æ®
        test_data_with_string = test_data.copy()
        test_data_with_string['category'] = ['A', 'B', 'C'] * 33 + ['A']

        # æµ‹è¯•éæ•°å€¼åˆ—
        zscore_standardization(test_data_with_string, 'category')
    except Exception as e:
        print(f"éæ•°å€¼åˆ—é”™è¯¯: {e}")

    try:
        # æµ‹è¯•ä¸å­˜åœ¨çš„åˆ—
        zscore_standardization(test_data, 'nonexistent_column')
    except Exception as e:
        print(f"ä¸å­˜åœ¨åˆ—é”™è¯¯: {e}")

    try:
        # æµ‹è¯•ç´¢å¼•è¶…å‡ºèŒƒå›´
        zscore_standardization(test_data, 10)
    except Exception as e:
        print(f"ç´¢å¼•è¶…å‡ºèŒƒå›´é”™è¯¯: {e}")

#-----------------------------------------------------------------------------------------------------------------------

"""
Min-Maxæ ‡å‡†åŒ–æ¨¡å—

åŠŸèƒ½ï¼šå°†æŒ‡å®šåˆ—çš„ç‰¹å¾ç¼©æ”¾åˆ°ç»™å®šèŒƒå›´ï¼ˆé€šå¸¸æ˜¯ [0, 1]ï¼‰ä¹‹é—´

å‚æ•°:
data: pd.DataFrameï¼Œè¾“å…¥çš„DataFrameæ•°æ®
columns: str/int/List[str/int]ï¼Œéœ€è¦æ ‡å‡†åŒ–çš„åˆ—åæˆ–åˆ—ç´¢å¼•
feature_range: List[float]ï¼Œå½’ä¸€åŒ–èŒƒå›´ï¼Œé»˜è®¤ [0, 1]

è¿”å›:
tuple: (æ ‡å‡†åŒ–åçš„DataFrame, æ ‡å‡†åŒ–å‚æ•°å­—å…¸)
å…¶ä¸­ï¼Œæ ‡å‡†åŒ–å‚æ•°å­—å…¸æ ¼å¼: {åˆ—å: {'min_val': æœ€å°å€¼, 'max_val': æœ€å¤§å€¼, 'scale': åŸå§‹èŒƒå›´}}

å¼‚å¸¸:
TypeError: è¾“å…¥ç±»å‹é”™è¯¯
ValueError: è¾“å…¥å€¼é”™è¯¯
KeyError: åˆ—åä¸å­˜åœ¨
IndexError: åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´
"""
# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_minmax_scaler():
    """æµ‹è¯•å‡½æ•°ï¼Œå±•ç¤ºæ¨¡å—çš„ä½¿ç”¨æ–¹æ³•"""
    print("=== Min-Maxæ ‡å‡†åŒ–æ¨¡å—å…¨é¢æµ‹è¯• ===\n")

    # åˆ›å»ºåŸºç¡€æµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'A': [1, 2, 3, 4, 5],
        'B': [10, 20, 30, 40, 50],
        'C': [100, 200, 300, 400, 500],
        'D': ['a', 'b', 'c', 'd', 'e']  # éæ•°å€¼åˆ—
    })

    print("åŸºç¡€æµ‹è¯•æ•°æ®:")
    print(test_data)
    print()

    # åŸºç¡€åŠŸèƒ½æµ‹è¯•
    print("=== åŸºç¡€åŠŸèƒ½æµ‹è¯• ===")
    try:
        # æµ‹è¯•1ï¼šæ ‡å‡†åŒ–å•åˆ—ï¼ˆä½¿ç”¨åˆ—åï¼‰
        print("æµ‹è¯•1ï¼šæ ‡å‡†åŒ–åˆ— 'A' (é»˜è®¤èŒƒå›´ [0,1])")
        result1, params1 = minmax_scaler(test_data, 'A')
        print("ç»“æœ:")
        print(result1[['A']])
        print("æ ‡å‡†åŒ–å‚æ•°:", params1)
        print()

        # æµ‹è¯•2ï¼šæ ‡å‡†åŒ–å¤šåˆ—ï¼ˆä½¿ç”¨åˆ—ç´¢å¼•ï¼‰
        print("æµ‹è¯•2ï¼šæ ‡å‡†åŒ–åˆ—ç´¢å¼• [0, 1] (èŒƒå›´ [-1,1])")
        result2, params2 = minmax_scaler(test_data, [0, 1], feature_range=[-1, 1])
        print("ç»“æœ:")
        print(result2[['A', 'B']])
        print("è¯´æ˜ï¼šå‡ºç°è´Ÿæ•°æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæŒ‡å®šäº†èŒƒå›´[-1,1]")
        print("æ ‡å‡†åŒ–å‚æ•°:", params2)
        print()

        # æµ‹è¯•3ï¼šæ··åˆä½¿ç”¨åˆ—åå’Œç´¢å¼•
        print("æµ‹è¯•3ï¼šæ··åˆä½¿ç”¨åˆ—åå’Œç´¢å¼•")
        result3, params3 = minmax_scaler(test_data, ['A', 2])
        print("ç»“æœ:")
        print(result3[['A', 'C']])
        print("æ ‡å‡†åŒ–å‚æ•°:", params3)
        print()

    except Exception as e:
        print(f"é”™è¯¯: {e}")

    # é«˜çº§åŠŸèƒ½æµ‹è¯•
    print("=== é«˜çº§åŠŸèƒ½æµ‹è¯• ===")

    # æµ‹è¯•4ï¼šåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®
    print("æµ‹è¯•4ï¼šåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®")
    data_with_nan = pd.DataFrame({
        'score': [85, 90, np.nan, 95, 80, np.nan, 88],
        'age': [25, np.nan, 30, 35, 28, 32, np.nan]
    })
    print("åŸå§‹æ•°æ®:")
    print(data_with_nan)
    try:
        result4, params4 = minmax_scaler(data_with_nan, ['score', 'age'])
        print("æ ‡å‡†åŒ–å:")
        print(result4)
        print("è¯´æ˜ï¼šç¼ºå¤±å€¼ä¿æŒä¸ºNaN")
        print()
    except Exception as e:
        print(f"é”™è¯¯: {e}")

    # æµ‹è¯•5ï¼šæµ®ç‚¹æ•°æ•°æ®
    print("æµ‹è¯•5ï¼šæµ®ç‚¹æ•°æ•°æ®")
    float_data = pd.DataFrame({
        'price': [19.99, 29.95, 15.50, 45.00, 12.99],
        'rating': [4.2, 3.8, 4.5, 3.9, 4.1]
    })
    print("åŸå§‹æ•°æ®:")
    print(float_data)
    try:
        result5, params5 = minmax_scaler(float_data, ['price', 'rating'], feature_range=[0, 10])
        print("æ ‡å‡†åŒ–å (èŒƒå›´ [0,10]):")
        print(result5)
        print()
    except Exception as e:
        print(f"é”™è¯¯: {e}")

    # æµ‹è¯•6ï¼šä¸åŒèŒƒå›´çš„æ ‡å‡†åŒ–
    print("æµ‹è¯•6ï¼šä¸åŒèŒƒå›´çš„æ ‡å‡†åŒ–")
    try:
        # æµ‹è¯•ä¸åŒçš„feature_range
        ranges_to_test = [[0, 1], [-1, 1], [0, 100], [-5, 5]]
        for range_val in ranges_to_test:
            result, params = minmax_scaler(test_data, 'A', feature_range=range_val)
            print(f"èŒƒå›´ {range_val}: {result['A'].tolist()}")
        print()
    except Exception as e:
        print(f"é”™è¯¯: {e}")

    # æµ‹è¯•7ï¼šå¤§æ•°æ®é›†æ€§èƒ½æµ‹è¯•
    print("æµ‹è¯•7ï¼šå¤§æ•°æ®é›†æ€§èƒ½æµ‹è¯•")
    large_data = pd.DataFrame({
        'values': np.random.randn(10000),
        'category': np.random.choice(['A', 'B', 'C'], 10000)
    })
    print(f"å¤§æ•°æ®é›†: {large_data.shape}")
    try:
        import time
        start_time = time.time()
        result7, params7 = minmax_scaler(large_data, 'values')
        end_time = time.time()
        print(f"å¤„ç†æ—¶é—´: {end_time - start_time:.4f}ç§’")
        print(f"ç»“æœç»Ÿè®¡: min={result7['values'].min():.4f}, max={result7['values'].max():.4f}")
        print()
    except Exception as e:
        print(f"é”™è¯¯: {e}")

    # å¼‚å¸¸æƒ…å†µæµ‹è¯•
    print("=== å¼‚å¸¸æƒ…å†µæµ‹è¯• ===")

    # æµ‹è¯•8ï¼šéæ•°å€¼åˆ—
    try:
        minmax_scaler(test_data, 'D')
    except ValueError as e:
        print(f"âœ“ éæ•°å€¼åˆ—å¼‚å¸¸æ•è·: {e}")

    # æµ‹è¯•9ï¼šä¸å­˜åœ¨çš„åˆ—
    try:
        minmax_scaler(test_data, 'X')
    except KeyError as e:
        print(f"âœ“ ä¸å­˜åœ¨åˆ—å¼‚å¸¸æ•è·: {e}")

    # æµ‹è¯•10ï¼šé”™è¯¯çš„feature_range
    try:
        minmax_scaler(test_data, 'A', feature_range=[1, 0])
    except ValueError as e:
        print(f"âœ“ é”™è¯¯èŒƒå›´å¼‚å¸¸æ•è·: {e}")

    # æµ‹è¯•11ï¼šå…¨ç¼ºå¤±å€¼æ•°æ®
    try:
        all_nan_data = pd.DataFrame({'col': [np.nan, np.nan, np.nan]})
        minmax_scaler(all_nan_data, 'col')
    except ValueError as e:
        print(f"âœ“ å…¨ç¼ºå¤±å€¼å¼‚å¸¸æ•è·: {e}")

    # æµ‹è¯•12ï¼šæ‰€æœ‰å€¼ç›¸åŒçš„æ•°æ®
    try:
        same_value_data = pd.DataFrame({'col': [5, 5, 5, 5]})
        minmax_scaler(same_value_data, 'col')
    except ValueError as e:
        print(f"âœ“ ç›¸åŒå€¼å¼‚å¸¸æ•è·: {e}")

    # æµ‹è¯•13ï¼šç©ºDataFrame
    try:
        empty_data = pd.DataFrame()
        minmax_scaler(empty_data, 'A')
    except ValueError as e:
        print(f"âœ“ ç©ºDataFrameå¼‚å¸¸æ•è·: {e}")

    # æµ‹è¯•14ï¼šé”™è¯¯çš„è¾“å…¥ç±»å‹
    try:
        minmax_scaler([1, 2, 3], 'A')
    except TypeError as e:
        print(f"âœ“ é”™è¯¯è¾“å…¥ç±»å‹å¼‚å¸¸æ•è·: {e}")

    # æµ‹è¯•15ï¼šåˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´
    try:
        minmax_scaler(test_data, 10)
    except IndexError as e:
        print(f"âœ“ åˆ—ç´¢å¼•è¶…èŒƒå›´å¼‚å¸¸æ•è·: {e}")

    # æµ‹è¯•16ï¼šæ··åˆæ•°æ®ç±»å‹
    print("\næµ‹è¯•16ï¼šæ··åˆæ•°æ®ç±»å‹å¤„ç†")
    mixed_data = pd.DataFrame({
        'numbers': [1, 2, 3, 4, 5],
        'mixed': [1, 'text', 3.5, 4, None]
    })
    print("æ··åˆæ•°æ®:")
    print(mixed_data)
    try:
        result16, params16 = minmax_scaler(mixed_data, 'mixed')
        print("å¤„ç†ç»“æœ:")
        print(result16[['mixed']])
    except ValueError as e:
        print(f"âœ“ æ··åˆæ•°æ®ç±»å‹å¼‚å¸¸æ•è·: {e}")

    print("\n=== æµ‹è¯•å®Œæˆ ===")
    print("è¯´æ˜ï¼š")
    print("1. æµ‹è¯•2å‡ºç°è´Ÿæ•°æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæŒ‡å®šäº†èŒƒå›´[-1,1]")
    print("2. Min-Maxæ ‡å‡†åŒ–å¯ä»¥å°†æ•°æ®ç¼©æ”¾åˆ°ä»»æ„æŒ‡å®šèŒƒå›´")
    print("3. ç¼ºå¤±å€¼ä¼šè¢«ä¿æŒä¸ºNaN")
    print("4. æ‰€æœ‰å¼‚å¸¸æƒ…å†µéƒ½å¾—åˆ°äº†æ­£ç¡®å¤„ç†")


if __name__ == "__main__":
    test_minmax_scaler()
#-----------------------------------------------------------------------------------------------------------------------
"""
å¯¹DataFrameè¿›è¡ŒRobustæ ‡å‡†åŒ–å¤„ç†ï¼ˆåŸºäºä¸­ä½æ•°å’ŒIQRï¼‰

å‚æ•°:
data : pd.DataFrameï¼Œè¾“å…¥çš„DataFrameæ•°æ®
columns : str, List[str], int, List[int]ï¼Œéœ€è¦æ ‡å‡†åŒ–çš„åˆ—åæˆ–åˆ—ç´¢å¼•ä½ç½®
quantile_range : List[float], default=[25, 75]ï¼ŒIQRèŒƒå›´çš„ç™¾åˆ†ä½æ•°ï¼Œé»˜è®¤ä½¿ç”¨25%-75%åˆ†ä½æ•°
inplace : bool, default=Falseï¼Œæ˜¯å¦åœ¨åŸDataFrameä¸Šç›´æ¥ä¿®æ”¹

è¿”å›:
å¦‚æœinplace=True: 
    è¿”å›ä¿®æ”¹åçš„DataFrameå’Œå˜æ¢å‚æ•°å­—å…¸
å¦‚æœinplace=False: 
    è¿”å›æ–°çš„DataFrameå’Œå˜æ¢å‚æ•°å­—å…¸

å¼‚å¸¸:
ValueError: è¾“å…¥å‚æ•°ä¸åˆæ³•æ—¶æŠ›å‡º
TypeError: æ•°æ®ç±»å‹ä¸åŒ¹é…æ—¶æŠ›å‡º
KeyError: åˆ—åä¸å­˜åœ¨æ—¶æŠ›å‡º
"""

# æµ‹è¯•é›†
def run_robust_standardization_tests():
    """
    è¿è¡Œrobustæ ‡å‡†åŒ–å‡½æ•°çš„å…¨é¢æµ‹è¯•é›†
    """
    print("=" * 70)
    print("ROBUSTæ ‡å‡†åŒ–å‡½æ•°æµ‹è¯•é›†")
    print("=" * 70)

    # æµ‹è¯•è®¡æ•°å™¨
    test_count = 0
    passed_count = 0

    def test_case(test_name, test_func):
        nonlocal test_count, passed_count
        test_count += 1
        print(f"\næµ‹è¯• {test_count}: {test_name}")
        print("-" * 50)
        try:
            test_func()
            print("âœ… æµ‹è¯•é€šè¿‡")
            passed_count += 1
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    # æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    def test_basic_functionality():
        np.random.seed(42)
        df = pd.DataFrame({
            'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
            'B': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
        })

        scaled_df, params = robust_standardization(df, columns=['A', 'B'])

        # éªŒè¯è¿”å›ç±»å‹
        assert isinstance(scaled_df, pd.DataFrame)
        assert isinstance(params, dict)

        # éªŒè¯åˆ—æ•°ä¸å˜
        assert len(scaled_df.columns) == len(df.columns)

        # éªŒè¯å‚æ•°ç»“æ„
        for col in ['A', 'B']:
            assert col in params
            assert 'median' in params[col]
            assert 'iqr' in params[col]
            assert 'q_low' in params[col]
            assert 'q_high' in params[col]

        print("åŸºæœ¬åŠŸèƒ½éªŒè¯é€šè¿‡")

    # æµ‹è¯•2: åˆ—åå’Œåˆ—ç´¢å¼•æ··åˆä½¿ç”¨
    def test_column_specification():
        df = pd.DataFrame({
            'col1': [1, 2, 3, 4, 5],
            'col2': [10, 20, 30, 40, 50],
            'col3': [100, 200, 300, 400, 500]
        })

        # ä½¿ç”¨åˆ—å
        scaled_df1, _ = robust_standardization(df, columns=['col1', 'col2'])

        # ä½¿ç”¨åˆ—ç´¢å¼•
        scaled_df2, _ = robust_standardization(df, columns=[0, 1])

        # æ··åˆä½¿ç”¨
        scaled_df3, _ = robust_standardization(df, columns=['col1', 1])

        # éªŒè¯ç»“æœä¸€è‡´
        pd.testing.assert_frame_equal(scaled_df1, scaled_df2)
        pd.testing.assert_frame_equal(scaled_df1, scaled_df3)

        print("åˆ—åå’Œç´¢å¼•æ··åˆä½¿ç”¨éªŒè¯é€šè¿‡")

    # æµ‹è¯•3: ç¼ºå¤±å€¼å¤„ç†
    def test_missing_values():
        df = pd.DataFrame({
            'A': [1, 2, np.nan, 4, 5, np.nan, 7, 8, 9, 10],
            'B': [10, np.nan, 30, 40, 50, 60, np.nan, 80, 90, 100]
        })

        scaled_df, params = robust_standardization(df, columns=['A', 'B'])

        # éªŒè¯ç¼ºå¤±å€¼ä½ç½®ä¿æŒä¸å˜
        assert scaled_df['A'].isna().sum() == df['A'].isna().sum()
        assert scaled_df['B'].isna().sum() == df['B'].isna().sum()

        # éªŒè¯ç¼ºå¤±å€¼ä½ç½®ç›¸åŒ
        pd.testing.assert_series_equal(scaled_df['A'].isna(), df['A'].isna())
        pd.testing.assert_series_equal(scaled_df['B'].isna(), df['B'].isna())

        print("ç¼ºå¤±å€¼å¤„ç†éªŒè¯é€šè¿‡")

    # æµ‹è¯•4: ç¦»ç¾¤å€¼é²æ£’æ€§
    def test_outlier_robustness():
        # åˆ›å»ºåŒ…å«ç¦»ç¾¤å€¼çš„æ•°æ®
        normal_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        outlier_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 1000]  # 1000æ˜¯ç¦»ç¾¤å€¼

        df_normal = pd.DataFrame({'A': normal_data})
        df_outlier = pd.DataFrame({'A': outlier_data})

        # æ ‡å‡†åŒ–
        scaled_normal, params_normal = robust_standardization(df_normal, columns=['A'])
        scaled_outlier, params_outlier = robust_standardization(df_outlier, columns=['A'])

        # Robustæ ‡å‡†åŒ–å¯¹ç¦»ç¾¤å€¼åº”è¯¥ä¸æ•æ„Ÿ
        # ä¸­ä½æ•°åº”è¯¥ç›¸åŒæˆ–æ¥è¿‘
        assert abs(params_normal['A']['median'] - params_outlier['A']['median']) < 1

        print("ç¦»ç¾¤å€¼é²æ£’æ€§éªŒè¯é€šè¿‡")

    # æµ‹è¯•5: ä¸åŒåˆ†ä½æ•°èŒƒå›´
    def test_quantile_ranges():
        df = pd.DataFrame({'A': np.random.normal(0, 1, 100)})

        # æµ‹è¯•ä¸åŒçš„åˆ†ä½æ•°èŒƒå›´
        ranges = [[10, 90], [25, 75], [5, 95]]

        for q_range in ranges:
            scaled_df, params = robust_standardization(df, columns=['A'], quantile_range=q_range)

            # éªŒè¯åˆ†ä½æ•°èŒƒå›´è¢«æ­£ç¡®è®°å½•
            assert params['A']['quantile_range'] == q_range

            # éªŒè¯IQR > 0
            assert params['A']['iqr'] > 0

        print("ä¸åŒåˆ†ä½æ•°èŒƒå›´éªŒè¯é€šè¿‡")

    # æµ‹è¯•6: inplaceå‚æ•°
    def test_inplace_parameter():
        df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]})
        df_original = df.copy()

        # æµ‹è¯•inplace=Falseï¼ˆé»˜è®¤ï¼‰
        scaled_df, _ = robust_standardization(df, columns=['A'])
        pd.testing.assert_frame_equal(df, df_original)  # åŸæ•°æ®ä¸å˜

        # æµ‹è¯•inplace=True
        scaled_df_inplace, _ = robust_standardization(df, columns=['A'], inplace=True)
        assert scaled_df_inplace is df  # è¿”å›åŒä¸€å¯¹è±¡
        assert not df.equals(df_original)  # åŸæ•°æ®å·²ä¿®æ”¹

        print("inplaceå‚æ•°éªŒè¯é€šè¿‡")

    # æµ‹è¯•7: å¼‚å¸¸æƒ…å†µå¤„ç†
    def test_exception_handling():
        df = pd.DataFrame({
            'A': [1, 2, 3, 4, 5],
            'B': ['a', 'b', 'c', 'd', 'e'],  # éæ•°å€¼åˆ—
            'C': [1, 1, 1, 1, 1]  # æ‰€æœ‰å€¼ç›¸åŒï¼ˆIQR=0ï¼‰
        })

        exception_tests = [
            # æµ‹è¯•ç”¨ä¾‹: (æµ‹è¯•å‡½æ•°, æœŸæœ›å¼‚å¸¸ç±»å‹, æè¿°)
            (lambda: robust_standardization([1, 2, 3], columns=['A']), TypeError, "éDataFrameè¾“å…¥"),
            (lambda: robust_standardization(pd.DataFrame(), columns=['A']), ValueError, "ç©ºDataFrame"),
            (lambda: robust_standardization(df, columns=['nonexistent']), KeyError, "ä¸å­˜åœ¨çš„åˆ—å"),
            (lambda: robust_standardization(df, columns=[10]), ValueError, "åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´"),
            (lambda: robust_standardization(df, columns=['B']), ValueError, "éæ•°å€¼åˆ—"),
            (lambda: robust_standardization(df, columns=['C']), ValueError, "IQRä¸º0"),
            (lambda: robust_standardization(df, columns=['A'], quantile_range=[75, 25]), ValueError, "æ— æ•ˆåˆ†ä½æ•°èŒƒå›´"),
        ]

        for test_func, expected_exception, description in exception_tests:
            try:
                test_func()
                assert False, f"{description} åº”è¯¥æŠ›å‡º {expected_exception.__name__}"
            except expected_exception:
                pass  # æ­£ç¡®æŠ›å‡ºäº†æœŸæœ›çš„å¼‚å¸¸
            except Exception as e:
                # å¦‚æœæŠ›å‡ºäº†å…¶ä»–å¼‚å¸¸ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯åŒ…è£…åçš„ValueError
                if expected_exception != ValueError or "ValueError" not in str(type(e)):
                    raise AssertionError(
                        f"{description} æœŸæœ›æŠ›å‡º {expected_exception.__name__}ï¼Œä½†æŠ›å‡ºäº† {type(e).__name__}: {e}")

        print("å¼‚å¸¸å¤„ç†éªŒè¯é€šè¿‡")

    # æµ‹è¯•8: è¾¹ç•Œæƒ…å†µ
    def test_edge_cases():
        # åªæœ‰ä¸¤è¡Œæ•°æ®
        df_two = pd.DataFrame({'A': [1, 2]})
        scaled_df, params = robust_standardization(df_two, columns=['A'])
        assert params['A']['median'] == 1.5
        # ä¸¤è¡Œæ•°æ®çš„IQRåº”è¯¥ä¸ä¸º0
        assert params['A']['iqr'] > 0

        # åªæœ‰ä¸‰è¡Œæ•°æ®ï¼ˆç¡®ä¿IQRä¸ä¸º0ï¼‰
        df_three = pd.DataFrame({'A': [1, 2, 3]})
        scaled_df, params = robust_standardization(df_three, columns=['A'])
        assert params['A']['median'] == 2.0
        assert params['A']['iqr'] > 0

        # åŒ…å«æå¤§å€¼
        df_extreme = pd.DataFrame({'A': [1, 2, 3, 1e10]})
        scaled_df, params = robust_standardization(df_extreme, columns=['A'])
        # ä¸­ä½æ•°åº”è¯¥ä¸å—æå€¼å½±å“
        assert params['A']['median'] == 2.5

        # æµ‹è¯•å•è¡Œæ•°æ®ä¼šæŠ›å‡ºå¼‚å¸¸ï¼ˆå› ä¸ºæ— æ³•è®¡ç®—æœ‰æ•ˆçš„IQRï¼‰
        df_single = pd.DataFrame({'A': [1]})
        try:
            robust_standardization(df_single, columns=['A'])
            assert False, "å•è¡Œæ•°æ®åº”è¯¥æŠ›å‡ºå¼‚å¸¸"
        except ValueError:
            pass  # æ­£ç¡®æŠ›å‡ºå¼‚å¸¸

        print("è¾¹ç•Œæƒ…å†µéªŒè¯é€šè¿‡")

    # æµ‹è¯•9: æ•°æ®ç±»å‹ä¿æŒ
    def test_data_type_preservation():
        df = pd.DataFrame({
            'int_col': [1, 2, 3, 4, 5],
            'float_col': [1.1, 2.2, 3.3, 4.4, 5.5],
            'mixed_col': [1, 2.5, 3, 4.7, 5]
        })

        scaled_df, _ = robust_standardization(df, columns=['int_col', 'float_col', 'mixed_col'])

        # æ ‡å‡†åŒ–ååº”è¯¥éƒ½æ˜¯floatç±»å‹
        for col in ['int_col', 'float_col', 'mixed_col']:
            assert pd.api.types.is_float_dtype(scaled_df[col])

        print("æ•°æ®ç±»å‹å¤„ç†éªŒè¯é€šè¿‡")

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_case("åŸºæœ¬åŠŸèƒ½æµ‹è¯•", test_basic_functionality)
    test_case("åˆ—åå’Œåˆ—ç´¢å¼•æ··åˆä½¿ç”¨", test_column_specification)
    test_case("ç¼ºå¤±å€¼å¤„ç†", test_missing_values)
    test_case("ç¦»ç¾¤å€¼é²æ£’æ€§", test_outlier_robustness)
    test_case("ä¸åŒåˆ†ä½æ•°èŒƒå›´", test_quantile_ranges)
    test_case("inplaceå‚æ•°", test_inplace_parameter)
    test_case("å¼‚å¸¸æƒ…å†µå¤„ç†", test_exception_handling)
    test_case("è¾¹ç•Œæƒ…å†µ", test_edge_cases)
    test_case("æ•°æ®ç±»å‹ä¿æŒ", test_data_type_preservation)

    # æµ‹è¯•æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    print(f"æ€»æµ‹è¯•æ•°: {test_count}")
    print(f"é€šè¿‡æµ‹è¯•æ•°: {passed_count}")
    print(f"å¤±è´¥æµ‹è¯•æ•°: {test_count - passed_count}")
    print(f"é€šè¿‡ç‡: {passed_count / test_count * 100:.1f}%")

    if passed_count == test_count:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä»£ç è´¨é‡è‰¯å¥½ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ä»£ç ã€‚")


# æ€§èƒ½æµ‹è¯•
def run_performance_test():
    """
    è¿è¡Œæ€§èƒ½æµ‹è¯•
    """
    print("\n" + "=" * 70)
    print("æ€§èƒ½æµ‹è¯•")
    print("=" * 70)

    import time

    # åˆ›å»ºå¤§æ•°æ®é›†
    np.random.seed(42)
    sizes = [1000, 10000, 100000]

    for size in sizes:
        df = pd.DataFrame({
            'A': np.random.normal(0, 1, size),
            'B': np.random.exponential(2, size),
            'C': np.random.uniform(0, 100, size)
        })

        # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
        nan_indices = np.random.choice(size, size // 20, replace=False)
        df.loc[nan_indices, 'A'] = np.nan

        start_time = time.time()
        scaled_df, params = robust_standardization(df, columns=['A', 'B', 'C'])
        end_time = time.time()

        print(f"æ•°æ®é‡: {size:,} è¡Œ Ã— 3 åˆ—")
        print(f"å¤„ç†æ—¶é—´: {end_time - start_time:.4f} ç§’")
        print(f"å¤„ç†é€Ÿåº¦: {size / (end_time - start_time):,.0f} è¡Œ/ç§’")
        print()

